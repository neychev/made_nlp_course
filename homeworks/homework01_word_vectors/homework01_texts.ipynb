{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 01. Simple text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic or not\n",
    "Your main goal in this assignment is to classify, whether the comments are toxic or not. And practice with both classical approaches and PyTorch in the process.\n",
    "\n",
    "*Credits: This homework is inspired by YSDA NLP_course.*\n",
    "\n",
    "*Disclaimer: The used dataset may contain obscene language and is used only as an example of real unfiltered data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab uncomment this cell\n",
    "# ! wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework01/utils.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('../../datasets/comments_small_dataset/comments.tsv', sep='\\t')\n",
    "except FileNotFoundError:\n",
    "    ! wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/comments_small_dataset/comments.tsv -nc\n",
    "    data = pd.read_csv(\"comments.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['comment_text'].values\n",
    "target = data['should_ban'].values\n",
    "data[50::200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(texts, target, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ it is generally a good idea to split data into train/test before anything is done to them.\n",
    "\n",
    "It guards you against possible data leakage in the preprocessing stage. For example, should you decide to select words present in obscene tweets as features, you should only count those words over the training set. Otherwise your algoritm can cheat evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and tokenization\n",
    "\n",
    "Comments contain raw text with punctuation, upper/lowercase letters and even newline symbols.\n",
    "\n",
    "To simplify all further steps, we'll split text into space-separated tokens using one of nltk tokenizers.\n",
    "\n",
    "Generally, library `nltk` [link](https://www.nltk.org) is widely used in NLP. It is not necessary in here, but mentioned to intoduce it to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n",
    "\n",
    "text = 'How to be a grown-up at work: replace \"I don\\'t want to do that\" with \"Ok, great!\".'\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", preprocess(text),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task: preprocess each comment in train and test\n",
    "\n",
    "texts_train = #<YOUR CODE>\n",
    "texts_test = #<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small check that everything is done properly\n",
    "assert texts_train[5] ==  'who cares anymore . they attack with impunity .'\n",
    "assert texts_test[89] == 'hey todds ! quick q ? why are you so gay'\n",
    "assert len(texts_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: bag of words\n",
    "\n",
    "One traditional approach to such problem is to use bag of words features:\n",
    "1. build a vocabulary of frequent words (use train data only)\n",
    "2. for each training sample, count the number of times a word occurs in it (for each word in vocabulary).\n",
    "3. consider this count a feature for some classifier\n",
    "\n",
    "__Note:__ in practice, you can compute such features using sklearn. __Please don't do that in the current assignment, though.__\n",
    "* `from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task: find up to k most frequent tokens in texts_train,\n",
    "# sort them by number of occurences (highest first)\n",
    "k = min(10000, len(set(' '.join(texts_train).split())))\n",
    "\n",
    "#<YOUR CODE>\n",
    "\n",
    "bow_vocabulary = #<YOUR CODE>\n",
    "\n",
    "print('example features:', sorted(bow_vocabulary)[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(text):\n",
    "    \"\"\" convert text string to an array of token counts. Use bow_vocabulary. \"\"\"\n",
    "    #<YOUR CODE>\n",
    "\n",
    "    return np.array(<...>, 'float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = np.stack(list(map(text_to_bow, texts_train)))\n",
    "X_test_bow = np.stack(list(map(text_to_bow, texts_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small check that everything is done properly\n",
    "k_max = len(set(' '.join(texts_train).split()))\n",
    "assert X_train_bow.shape == (len(texts_train), min(k, k_max))\n",
    "assert X_test_bow.shape == (len(texts_test), min(k, k_max))\n",
    "assert np.all(X_train_bow[5:10].sum(-1) == np.array([len(s.split()) for s in  texts_train[5:10]]))\n",
    "assert len(bow_vocabulary) <= min(k, k_max)\n",
    "assert X_train_bow[6, bow_vocabulary.index('.')] == texts_train[6].split().count('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the trick with `sklearn` logistic regression implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_model = LogisticRegression().fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "for name, X, y, model in [\n",
    "    ('train', X_train_bow, y_train, bow_model),\n",
    "    ('test ', X_test_bow, y_test, bow_model)\n",
    "]:\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    auc = roc_auc_score(y, proba)\n",
    "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems alright. Now let's create the simple logistic regression using PyTorch. Just like in the classwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_train_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('l1', ### YOUR CODE HERE\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember what we discussed about loss functions! `nn.CrossEntropyLoss` combines both log-softmax and `NLLLoss`.\n",
    "\n",
    "__Be careful with it! Criterion `nn.CrossEntropyLoss` with still work with log-softmax output, but it won't allow you to converge to the optimum.__ Next comes small demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.NLLLoss()\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow_torch = ### YOUR CODE HERE\n",
    "X_test_bow_torch = ### YOUR CODE HERE\n",
    "\n",
    "y_train_torch = ### YOUR CODE HERE\n",
    "y_test_torch = ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that everything is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example loss\n",
    "loss = loss_function(model(X_train_bow_torch[:3]), y_train_torch[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(loss.item()) == float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes small function to train the model. In future we will take in into separate file, but for this homework it's ok to implement it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    opt,\n",
    "    lr_scheduler,\n",
    "    X_train_torch,\n",
    "    y_train_torch,\n",
    "    X_val_torch,\n",
    "    y_val_torch,\n",
    "    n_iterations=500,\n",
    "    batch_size=32,\n",
    "    warm_start=False,\n",
    "    show_plots=True,\n",
    "    eval_every=10\n",
    "):\n",
    "    if not warm_start:\n",
    "        for name, module in model.named_children():\n",
    "            print('resetting ', name)\n",
    "            try:\n",
    "                module.reset_parameters()\n",
    "            except AttributeError as e:\n",
    "                print('Cannot reset {} module parameters: {}'.format(name, e))\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    local_train_loss_history = []\n",
    "    local_train_acc_history = []\n",
    "    for i in range(n_iterations):\n",
    "\n",
    "        # sample 256 random observations\n",
    "        ix = np.random.randint(0, len(X_train_torch), batch_size)\n",
    "        x_batch = X_train_torch[ix]\n",
    "        y_batch = y_train_torch[ix]\n",
    "\n",
    "        # predict log-probabilities or logits\n",
    "        y_predicted = ### YOUR CODE\n",
    "\n",
    "        # compute loss, just like before\n",
    "        ### YOUR CODE\n",
    "\n",
    "\n",
    "        # compute gradients\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Adam step\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # clear gradients\n",
    "        ### YOUR CODE\n",
    "\n",
    "\n",
    "        local_train_loss_history.append(loss.data.numpy())\n",
    "        local_train_acc_history.append(\n",
    "            accuracy_score(\n",
    "                y_batch.to('cpu').detach().numpy(),\n",
    "                y_predicted.to('cpu').detach().numpy().argmax(axis=1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if i % eval_every == 0:\n",
    "            train_loss_history.append(np.mean(local_train_loss_history))\n",
    "            train_acc_history.append(np.mean(local_train_acc_history))\n",
    "            local_train_loss_history, local_train_acc_history = [], []\n",
    "\n",
    "            predictions_val = model(X_val_torch)\n",
    "            val_loss_history.append(loss_function(predictions_val, y_val_torch).to('cpu').detach().item())\n",
    "\n",
    "            acc_score_val = accuracy_score(y_val_torch.cpu().numpy(), predictions_val.to('cpu').detach().numpy().argmax(axis=1))\n",
    "            val_acc_history.append(acc_score_val)\n",
    "            lr_scheduler.step(train_loss_history[-1])\n",
    "\n",
    "            if show_plots:\n",
    "                display.clear_output(wait=True)\n",
    "                plot_train_process(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on the data. Note, that here we use the `test` part of the data for validation. It's not so good idea in general, but in this task our main goal is practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, opt, lr_scheduler, X_train_bow_torch, y_train_torch, X_test_bow_torch, y_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "for name, X, y, model in [\n",
    "    ('train', X_train_bow_torch, y_train, model),\n",
    "    ('test ', X_test_bow_torch, y_test, model)\n",
    "]:\n",
    "    proba = model(X).detach().cpu().numpy()[:, 1]\n",
    "    auc = roc_auc_score(y, proba)\n",
    "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to vary the number of tokens `k` and check how the model performance changes. Show it on a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: implement TF-IDF features\n",
    "\n",
    "Not all words are equally useful. One can prioritize rare words and downscale words like \"and\"/\"or\" by using __tf-idf features__. This abbreviation stands for __text frequency/inverse document frequence__ and means exactly that:\n",
    "\n",
    "$$ feature_i = { Count(word_i \\in x) \\times { log {N \\over Count(word_i \\in D) + \\alpha} }}, $$\n",
    "\n",
    "\n",
    "where x is a single text, D is your dataset (a collection of texts), N is a total number of documents and $\\alpha$ is a smoothing hyperparameter (typically 1). \n",
    "And $Count(word_i \\in D)$ is the number of documents where $word_i$ appears.\n",
    "\n",
    "It may also be a good idea to normalize each data sample after computing tf-idf features.\n",
    "\n",
    "__Your task:__ implement tf-idf features, train a model and evaluate ROC curve. Compare it with basic BagOfWords model from above.\n",
    "\n",
    "__Please don't use sklearn/nltk builtin tf-idf vectorizers in your solution :)__ You can still use 'em for debugging though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog post about implementing the TF-IDF features from scratch: https://triton.ml/blog/tf-idf-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same stuff about moel and optimizers here (or just omit it, if you are using the same model as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_torch = ### YOUR CODE HERE\n",
    "X_test_tfidf_torch = ### YOUR CODE HERE\n",
    "\n",
    "y_train_torch = ### YOUR CODE HERE\n",
    "y_test_torch = ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your model to the data. No not hesitate to vary number of iterations, learning rate and so on.\n",
    "\n",
    "_Note: due to very small dataset, increasing the complexity of the network might not be the best idea._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Comparing it with Naive Bayes\n",
    "\n",
    "Naive Bayes classifier is a good choice for such small problems. Try to tune it for both BOW and TF-iDF features. Compare the results with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape some thoughts on the results you aquired. Which model has show the best performance? Did changing the learning rate/lr scheduler help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your beautiful thoughts here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Using the external knowledge.\n",
    "\n",
    "Use the `gensim` word2vec pretrained model to translate words into vectors. Use several models with this new encoding technique. Compare the results, share your thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
