Positional Encoding illustrated:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/made_nlp_course/blob/spring2021/week06_Transformer_and_NMT_practice/positional_encoding_carriers.ipynb)


Neural Machine Translation as seq2sec:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/made_nlp_course/blob/spring2021/week06_Transformer_and_NMT_practice/practice_seq2seq_NMT_and_tensorboard.ipynb)

Further readings:

* Great blog post by Jay Alammar: https://jalammar.github.io/illustrated-transformer/
* Notebook on positional encoding: [link](https://github.com/ml-mipt/ml-mipt/blob/advanced/week04_Transformer/week04_positional_encoding_carriers.ipynb)
* Great Annotated Transformer article with code and comments by Harvard NLP group: https://nlp.seas.harvard.edu/2018/04/03/attention.html
